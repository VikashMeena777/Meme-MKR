name: Original Meme Factory ‚Äî Video Processor

on:
  workflow_dispatch:
    inputs:
      video_url:
        description: 'Reddit video URL'
        required: true
      video_id:
        description: 'Unique video ID'
        required: true
      reddit_title:
        description: 'Reddit post title'
        required: true
      reddit_sub:
        description: 'Subreddit name'
        required: false
        default: ''

  repository_dispatch:
    types: [process-meme]

jobs:
  process-meme:
    runs-on: ubuntu-latest
    timeout-minutes: 90  # Safety timeout per video

    steps:
      # ‚îÄ‚îÄ Setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # Cache HuggingFace models (BLIP-large ~1.8GB, Whisper small ~461MB)
      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface
          key: hf-models-blip-large-whisper-small-v1
          restore-keys: |
            hf-models-blip-large-whisper-small-
            hf-models-

      # Cache Whisper model separately (stored in ~/.cache/whisper)
      - name: Cache Whisper model
        uses: actions/cache@v4
        with:
          path: ~/.cache/whisper
          key: whisper-small-v1

      # Cache pip packages
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ hashFiles('scripts/requirements.txt') }}
          restore-keys: pip-

      # ‚îÄ‚îÄ Install Dependencies ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg
          # Install fonts: DejaVu for Latin, Noto for Hindi/Devanagari/multilingual
          sudo apt-get install -y fonts-dejavu-core fonts-liberation fonts-noto fonts-noto-cjk

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r scripts/requirements.txt

      # ‚îÄ‚îÄ Setup Cookies (for Reddit video downloads) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Setup cookies
        run: |
          touch cookies.txt
          if [ -n "${{ secrets.REDDIT_COOKIES }}" ]; then
            COOKIES="${{ secrets.REDDIT_COOKIES }}"
            
            # Check if it looks like Netscape cookie format (starts with # or domain)
            if echo "$COOKIES" | head -1 | grep -qE '^(#|\.reddit|reddit)'; then
              echo "Cookies appear to be in plain text Netscape format"
              echo "$COOKIES" > cookies.txt
            else
              # Try base64 decoding
              echo "Trying base64 decode..."
              echo "$COOKIES" | base64 -d > cookies.txt 2>/dev/null || {
                echo "Base64 failed, using as plain text"
                echo "$COOKIES" > cookies.txt
              }
            fi
            
            # Validate cookies file
            COOKIE_LINES=$(grep -v '^#' cookies.txt | grep -v '^$' | wc -l)
            echo "‚úÖ Reddit cookies configured ($COOKIE_LINES cookie entries)"
            
            # Debug: show first line format (redacted)
            head -1 cookies.txt | cut -c1-30
          else
            echo "‚ö†Ô∏è No REDDIT_COOKIES secret set ‚Äî Reddit downloads will likely fail!"
            echo "Add your Reddit cookies.txt content as REDDIT_COOKIES repository secret"
          fi

      # ‚îÄ‚îÄ Set Video Info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Set video info from dispatch
        id: video-info
        run: |
          if [ "${{ github.event_name }}" == "repository_dispatch" ]; then
            echo "video_url=${{ github.event.client_payload.video_url }}" >> $GITHUB_OUTPUT
            echo "reddit_video_url=${{ github.event.client_payload.reddit_video_url }}" >> $GITHUB_OUTPUT
            echo "video_id=${{ github.event.client_payload.video_id }}" >> $GITHUB_OUTPUT
            echo "reddit_title=${{ github.event.client_payload.reddit_title }}" >> $GITHUB_OUTPUT
            echo "reddit_sub=${{ github.event.client_payload.reddit_sub }}" >> $GITHUB_OUTPUT
          else
            echo "video_url=${{ github.event.inputs.video_url }}" >> $GITHUB_OUTPUT
            echo "reddit_video_url=" >> $GITHUB_OUTPUT
            echo "video_id=${{ github.event.inputs.video_id }}" >> $GITHUB_OUTPUT
            echo "reddit_title=${{ github.event.inputs.reddit_title }}" >> $GITHUB_OUTPUT
            echo "reddit_sub=${{ github.event.inputs.reddit_sub }}" >> $GITHUB_OUTPUT
          fi

      # ‚îÄ‚îÄ Run Processing Pipeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Process video through meme pipeline
        env:
          VIDEO_URL: ${{ steps.video-info.outputs.video_url }}
          REDDIT_VIDEO_URL: ${{ steps.video-info.outputs.reddit_video_url }}
          VIDEO_ID: ${{ steps.video-info.outputs.video_id }}
          REDDIT_TITLE: ${{ steps.video-info.outputs.reddit_title }}
          REDDIT_SUB: ${{ steps.video-info.outputs.reddit_sub }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          N8N_WEBHOOK_URL: ${{ secrets.N8N_WEBHOOK_URL }}
        run: |
          python scripts/process_video.py

      # ‚îÄ‚îÄ Save Debug Output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Upload processing output
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: meme-output-${{ steps.video-info.outputs.video_id }}
          path: output/
          retention-days: 7

      # ‚îÄ‚îÄ Cleanup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Cleanup
        if: always()
        run: |
          rm -rf work/
          rm -f cookies.txt
          echo "üßπ Cleanup complete"


  # ‚îÄ‚îÄ Batch Processing Job (process up to 3 videos) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  batch-process:
    runs-on: ubuntu-latest
    timeout-minutes: 330  # 5.5 hours max for batch

    # Only runs via repository_dispatch with batch type
    if: github.event_name == 'repository_dispatch' && github.event.action == 'process-meme-batch'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface
          key: hf-models-blip-large-whisper-small-v1

      - name: Cache Whisper model
        uses: actions/cache@v4
        with:
          path: ~/.cache/whisper
          key: whisper-small-v1

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ hashFiles('scripts/requirements.txt') }}

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg fonts-dejavu-core fonts-liberation
          pip install --upgrade pip
          pip install -r scripts/requirements.txt

      - name: Setup cookies
        run: |
          touch cookies.txt
          if [ -n "${{ secrets.REDDIT_COOKIES }}" ]; then
            echo "${{ secrets.REDDIT_COOKIES }}" >> cookies.txt
          fi

      - name: Process batch of videos
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          N8N_WEBHOOK_URL: ${{ secrets.N8N_WEBHOOK_URL }}
          VIDEOS_JSON: ${{ toJSON(github.event.client_payload.videos) }}
        run: |
          echo "Processing batch of videos..."
          echo "$VIDEOS_JSON" | python -c "
          import json, sys, os, subprocess

          videos = json.load(sys.stdin)
          max_videos = 3  # Safety limit

          for i, video in enumerate(videos[:max_videos]):
              print(f'\nüé¨ Processing video {i+1}/{min(len(videos), max_videos)}')
              env = os.environ.copy()
              env['VIDEO_URL'] = video['video_url']
              env['VIDEO_ID'] = video['video_id']
              env['REDDIT_TITLE'] = video.get('reddit_title', '')
              env['REDDIT_SUB'] = video.get('reddit_sub', '')

              result = subprocess.run(
                  ['python', 'scripts/process_video.py'],
                  env=env, timeout=5400  # 90 min per video
              )
              if result.returncode != 0:
                  print(f'  ‚ö†Ô∏è Video {video[\"video_id\"]} failed, continuing...')
              # Clean work dir between videos
              subprocess.run(['rm', '-rf', 'work/'])
          "

      - name: Upload all outputs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: batch-output
          path: output/
          retention-days: 7

      - name: Cleanup
        if: always()
        run: |
          rm -rf work/
          rm -f cookies.txt
